# -*- coding: utf-8 -*-
"""langchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gps9NGd_lWAqP46debm-7IVPsagcPTBH
"""

import langchain

#!pip install langchain_openai

from langchain_openai import ChatOpenAI

import os
openai_api_key = os.getenv("OPENAI_API_KEY")

from langchain_openai import ChatOpenAI
llm = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-3.5-turbo")
llm_response = llm.invoke("dubai famaous location")
llm_response

from langchain_core.output_parsers import StrOutputParser
output_parsers = StrOutputParser()
output_parsers.invoke(llm_response)

chain = llm | output_parsers
chain.invoke("AbuDhabi best place")

from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_template("tell me about {topic}")
chain = prompt | llm | output_parsers
result = chain.invoke({"topic":"India summer season"})
print(result)

#! pip install langchain_community

#/var/dummy_ai_document.pdf
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing import List
from langchain_core.documents import Document

from google.colab import files
uploaded = files.upload()

file_name = list(uploaded.keys())[0]  # Get the uploaded filename
file_path = file_name
print(file_path)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200, length_function=len)

loader = PyPDFLoader(file_path)

#! pip install pypdf
loaded_docs = loader.load()
print(len(loaded_docs))

splits = text_splitter.split_documents(loaded_docs)
print(f"split docs into {len(splits)} chunks")

from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
document_embeddings=embeddings.embed_documents([split.page_content for split in splits])
print(f"creating embeddings for {len(document_embeddings)} for chunks")
import pinecone
pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_env = os.getenv("PINECONE_REGION")
pc = pinecone.Pinecone(api_key=pinecone_api_key)
index = pc.Index("task1")
vectors_to_upsert = []
for i, (split, embedding) in enumerate(zip(splits, document_embeddings)):
    vector_id = f"chunk-{i}"  # Unique id
    metadata = {
        "text": split.page_content  # <-- VERY IMPORTANT
    }
    vectors_to_upsert.append({
        "id": vector_id,
        "values": embedding,
        "metadata": metadata
    })

index.upsert(vectors=vectors_to_upsert)
print(f"Successfully stored {len(vectors_to_upsert)} vectors into Pinecone!")

from langchain_pinecone import PineconeVectorStore
from langchain.chains import RetrievalQA
# 6. Connect to existing Pinecone index
pinecone_api_key = os.getenv("PINECONE_API_KEY")
vectorstore = PineconeVectorStore(
    index_name="task1",
    embedding=embeddings,
    pinecone_api_key=pinecone_api_key
)

# 7. Initialize GPT-3.5 LLM
llm = ChatOpenAI(
    openai_api_key=openai_api_key,
    model="gpt-3.5-turbo"
)

# 8. Create RAG Pipeline (Retriever + LLM)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    chain_type="stuff"
)

query = "What are the key points mentioned in the uploaded document?"
response = qa_chain.invoke({"query": query})

print(response["result"])